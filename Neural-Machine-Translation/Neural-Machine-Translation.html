<head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160773620-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-160773620-1');
        </script>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <!-- Fontawesome -->
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.2/css/all.css" integrity="sha384-/rXc/GQVaYpyDdyxK+ecHPVYJSN9bmVFBvjA/9eOB+pb3F2w2N6fc5qB9Ew5yIns" crossorigin="anonymous">

        <!-- Google Fonts -->
        <link href="https://fonts.googleapis.com/css?family=Lexend+Deca|Noto+Sans:600|Work+Sans:400,500|Playfair+Display+SC:700&display=swap" rel="stylesheet">

        <!-- Bootstrap CSS -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

        <!-- CSS sheets -->
        <link rel="stylesheet" href="../style/index.css?v=<?=time();?>" type="text/css"/>
        <title>Timothy Delille</title>

  <style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
      
      
#notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
      .input_prompt{
          display:none;
      }
  </style>
  

  <!-- Loading mathjax macro -->
  <!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration -->
</head>
<html>
<body class='bg-light'>
<nav class="navbar navbar-expand-lg navbar-light scroll-fade">
  <img class='mx-auto' src='../Delille_Timothy.jpg' style='width:3rem;border-radius:50%;'/>
  <h3 class='ml-2'><a href='../index.html'>Timothy Delille</a></h3>
  <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNavDropdown">
    <ul class="navbar-nav ml-auto">
      <li class="nav-item active">
        <a class="nav-link" href="../index.html">Work <span class="sr-only">(current)</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="about.html">About</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="../Resume - Timothy Delille.pdf">Resume</a>
      </li>
    </ul>
  </div>
</nav>
<div id="particles-js" class='col d-none d-md-block' style='height:10rem;'></div>
    
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Neural-Machine-Translation">Neural Machine Translation<a class="anchor-link" href="#Neural-Machine-Translation">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
    <p>We build a neural machine translation system using modern techniques for sequence-to-sequence modeling. We first implement a baseline encoder-decoder architecture, then improve upon the baseline by adding an attention mechanism and implementing beam search. The end result is a fully functional translation system capable of translating simple German sentences into English. This project is part of the <em>CS288 - AI for Natural Language Processing</em> class at University of California, Berkeley. <a href='https://github.com/TimothyDelille/Neural-Machine-Translation'>Github link</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">Setup<a class="anchor-link" href="#Setup">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we install and import the required dependencies. These include:</p>
<ul>
<li><code>torch</code> for modeling and training</li>
<li><code>torchtext</code> for data collection</li>
<li><code>sentencepiece</code> for subword tokenization</li>
<li><code>sacrebleu</code> for BLEU score evaluation</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">sacrebleu</span> <span class="n">sentencepiece</span> <span class="n">torch</span> <span class="n">torchtext</span> <span class="n">tqdm</span>

<span class="c1"># Standard library imports</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Third party imports</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sacrebleu</span>
<span class="kn">import</span> <span class="nn">sentencepiece</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchtext</span>
<span class="kn">import</span> <span class="nn">tqdm.notebook</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before proceeding, let's verify that we're connected to a GPU runtime and that <code>torch</code> can detect the GPU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using device:&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Using device: cuda
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data">Data<a class="anchor-link" href="#Data">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The data for this assignment comes from the <a href="https://arxiv.org/abs/1605.00459">Multi30K dataset</a>, which contains English and German captions for images from Flickr. We can download and unpack it using <code>torchtext</code>. We use the Multi30K dataset because it is simpler than standard translation benchmark datasets and allows for models to be trained and evaluated in a matter of minutes rather than days.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">extensions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.de&quot;</span><span class="p">,</span> <span class="s2">&quot;.en&quot;</span><span class="p">]</span>
<span class="n">source_field</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">tokenize</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
<span class="n">target_field</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">tokenize</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
<span class="n">training_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">Multi30k</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>
    <span class="n">extensions</span><span class="p">,</span> <span class="p">[</span><span class="n">source_field</span><span class="p">,</span> <span class="n">target_field</span><span class="p">],</span> <span class="n">root</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>training.tar.gz:   0%|          | 0.00/1.21M [00:00&lt;?, ?B/s]</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>downloading training.tar.gz
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00&lt;00:00, 6.83MB/s]
validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00&lt;00:00, 1.75MB/s]</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>downloading validation.tar.gz
downloading mmt_task1_test2016.tar.gz
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00&lt;00:00, 1.65MB/s]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Vocabulary">Vocabulary<a class="anchor-link" href="#Vocabulary">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use <code>sentencepiece</code> to create a joint German-English subword vocabulary from the training corpus. Because the number of training examples is small, we choose a smaller vocabulary size than would be used for large-scale Neural Machine Translation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;pad_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;bos_id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;eos_id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;unk_id&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;multi30k/train.de,multi30k/train.en&quot;</span><span class="p">,</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">8000</span><span class="p">,</span>
    <span class="s2">&quot;model_prefix&quot;</span><span class="p">:</span> <span class="s2">&quot;multi30k&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">combined_args</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="s2">&quot;--</span><span class="si">{}</span><span class="s2">=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">sentencepiece</span><span class="o">.</span><span class="n">SentencePieceTrainer</span><span class="o">.</span><span class="n">Train</span><span class="p">(</span><span class="n">combined_args</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[5]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This creates two files: <code>multi30k.model</code> and <code>multi30k.vocab</code>. The first is a binary file containing the relevant data for the vocabulary. The second is a human-readable listing of each subword and its associated score.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The vocabulary consists of four special tokens (<code>&lt;pad&gt;</code> for padding, <code>&lt;s&gt;</code> for beginning of sentence (BOS), <code>&lt;/s&gt;</code> for end of sentence (EOS), <code>&lt;unk&gt;</code> for unknown) and a mixture of German and English words and subwords.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To use the vocabulary, we first need to load it from the binary file produced above.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">sentencepiece</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
<span class="n">vocab</span><span class="o">.</span><span class="n">Load</span><span class="p">(</span><span class="s2">&quot;multi30k.model&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[7]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The vocabulary object includes a number of methods for working with full sequences or individual pieces. A complete interface can be found on <a href="https://github.com/google/sentencepiece/tree/master/python#usage">GitHub</a> for reference.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We define some constants here for the first three special tokens that will become useful in the following sections.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pad_id</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">PieceToId</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">)</span>
<span class="n">bos_id</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">PieceToId</span><span class="p">(</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">)</span>
<span class="n">eos_id</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">PieceToId</span><span class="p">(</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that these tokens will be stripped from the output when converting from word pieces to text. This will be helpful when implementing greedy search and beam search.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Baseline-sequence-to-sequence-model">Baseline sequence-to-sequence model<a class="anchor-link" href="#Baseline-sequence-to-sequence-model">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With our data and vocabulary loaded, we're now ready to build a baseline sequence-to-sequence model.  Later on we'll add an attention mechanism to the model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's begin by defining a batch iterator for the training data. Given a dataset and a batch size, it will iterate over the dataset and yield pairs of tensors containing the subword indices for the source and target sentences in the batch, respectively.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">make_batch</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert a list of sentences into a batch of subword indices.</span>

<span class="sd">    Args:</span>
<span class="sd">        sentences: A list of sentences, each of which is a string.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A LongTensor of size (max_sequence_length, batch_size) containing the</span>
<span class="sd">        subword indices for the sentences, where max_sequence_length is the length</span>
<span class="sd">        of the longest sentence as encoded by the subword vocabulary and batch_size</span>
<span class="sd">        is the number of sentences in the batch. A beginning-of-sentence token</span>
<span class="sd">        should be included before each sequence, and an end-of-sentence token should</span>
<span class="sd">        be included after each sequence. Empty slots at the end of shorter sequences</span>
<span class="sd">        should be filled with padding tokens. The tensor should be located on the</span>
<span class="sd">        device defined at the beginning of the notebook.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">bos_id</span><span class="p">]</span> <span class="o">+</span> <span class="n">vocab</span><span class="o">.</span><span class="n">EncodeAsIds</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">eos_id</span><span class="p">])</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">padding_value</span> <span class="o">=</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sequences</span>

<span class="k">def</span> <span class="nf">make_batch_iterator</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a batch iterator that yields source-target pairs.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: A torchtext dataset object.</span>
<span class="sd">        batch_size: An integer batch size.</span>
<span class="sd">        shuffle: A boolean indicating whether to shuffle the examples.</span>

<span class="sd">    Yields:</span>
<span class="sd">        Pairs of tensors constructed by calling the make_batch function on the</span>
<span class="sd">        source and target sentences in the current group of examples. The max</span>
<span class="sd">        sequence length can differ between the source and target tensor, but the</span>
<span class="sd">        batch size will be the same. The final batch may be smaller than the given</span>
<span class="sd">        batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">examples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">start_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">example_batch</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="n">source_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">src</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">example_batch</span><span class="p">]</span>
        <span class="n">target_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">trg</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">example_batch</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">source_sentences</span><span class="p">),</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">target_sentences</span><span class="p">)</span>

<span class="n">test_batch</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">([</span><span class="s2">&quot;a test input&quot;</span><span class="p">,</span> <span class="s2">&quot;a longer input than the first&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example batch tensor:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">test_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span>
<span class="k">assert</span> <span class="n">test_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">bos_id</span>
<span class="k">assert</span> <span class="n">test_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">bos_id</span>
<span class="k">assert</span> <span class="n">test_batch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">pad_id</span>
<span class="k">assert</span> <span class="n">test_batch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">eos_id</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Example batch tensor:
tensor([[   1,    1],
        [   5,    5],
        [3980,  352],
        [   6,   60],
        [ 234,    6],
        [ 760,  234],
        [   2,  760],
        [   0, 5335],
        [   0,   13],
        [   0, 3769],
        [   0,    2]], device=&#39;cuda:0&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we will define the model itself. It should consist of a bidirectional LSTM encoder that encodes the input sentence into a fixed-size representation, and an LSTM decoder that uses this representation to produce the output sentence.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Seq2seqBaseline</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">GetPieceSize</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lstm_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">bidirectional</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lstm_decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Encode the source batch using a bidirectional LSTM encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">      source: An integer tensor with shape (max_source_sequence_length,</span>
<span class="sd">        batch_size) containing subword indices for the source sentences.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple with three elements:</span>
<span class="sd">        encoder_output: The output of the bidirectional LSTM with shape</span>
<span class="sd">          (max_source_sequence_length, batch_size, 2 * hidden_size).</span>
<span class="sd">        encoder_mask: A boolean tensor with shape (max_source_sequence_length,</span>
<span class="sd">          batch_size) indicating which encoder outputs correspond to padding</span>
<span class="sd">          tokens. Its elements should be True at positions corresponding to</span>
<span class="sd">          padding tokens and False elsewhere.</span>
<span class="sd">        encoder_hidden: The final hidden states of the bidirectional LSTM (after</span>
<span class="sd">          a suitable projection) that will be used to initialize the decoder.</span>
<span class="sd">          This should be a pair of tensors (h_n, c_n), each with shape</span>
<span class="sd">          (num_layers, batch_size, hidden_size). Note that the hidden state</span>
<span class="sd">          returned by the LSTM cannot be used directly. Its initial dimension is</span>
<span class="sd">          twice the required size because it contains state from two directions.</span>

<span class="sd">    The first two return values are not required for the baseline model and will</span>
<span class="sd">    only be used later in the attention model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">source</span> <span class="o">!=</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">h_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">c_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1">#have to define on the cuda device or colab will crash</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h_0</span><span class="p">,</span> <span class="n">c_0</span><span class="p">))</span>
    <span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span> <span class="o">=</span> <span class="n">hidden</span>
    <span class="n">h_n</span> <span class="o">=</span> <span class="n">h_n</span><span class="p">[:</span><span class="mi">2</span><span class="p">,:,:]</span> <span class="o">+</span> <span class="n">h_n</span><span class="p">[</span><span class="mi">2</span><span class="p">:,:,:]</span>
    <span class="n">c_n</span> <span class="o">=</span> <span class="n">c_n</span><span class="p">[:</span><span class="mi">2</span><span class="p">,:,:]</span> <span class="o">+</span> <span class="n">c_n</span><span class="p">[</span><span class="mi">2</span><span class="p">:,:,:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">source</span> <span class="o">==</span> <span class="n">pad_id</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">,</span> <span class="n">initial_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run the decoder LSTM starting from an initial hidden state.</span>

<span class="sd">    The third and fourth arguments are not used in the baseline model, but are</span>
<span class="sd">    included for compatibility with the attention model in the next section.</span>

<span class="sd">    Args:</span>
<span class="sd">      decoder_input: An integer tensor with shape (max_decoder_sequence_length,</span>
<span class="sd">        batch_size) containing the subword indices for the decoder input. During</span>
<span class="sd">        evaluation, where decoding proceeds one step at a time, the initial</span>
<span class="sd">        dimension should be 1.</span>
<span class="sd">      initial_hidden: A pair of tensors (h_0, c_0) representing the initial</span>
<span class="sd">        state of the decoder, each with shape (num_layers, batch_size,</span>
<span class="sd">        hidden_size).</span>
<span class="sd">      encoder_output: The output of the encoder with shape</span>
<span class="sd">        (max_source_sequence_length, batch_size, 2 * hidden_size).</span>
<span class="sd">      encoder_mask: The output mask from the encoder with shape</span>
<span class="sd">        (max_source_sequence_length, batch_size). Encoder outputs at positions</span>
<span class="sd">        with a True value correspond to padding tokens and should be ignored.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple with three elements:</span>
<span class="sd">        logits: A tensor with shape (max_decoder_sequence_length, batch_size,</span>
<span class="sd">          vocab_size) containing unnormalized scores for the next-word</span>
<span class="sd">          predictions at each position.</span>
<span class="sd">        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as</span>
<span class="sd">          initial_hidden representing the updated decoder state after processing</span>
<span class="sd">          the decoder input.</span>
<span class="sd">        attention_weights: This will be implemented later in the attention</span>
<span class="sd">          model, but in order to maintain compatible type signatures, we also</span>
<span class="sd">          include it here.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># These arguments are not used in the baseline model.</span>
    <span class="k">del</span> <span class="n">encoder_output</span>
    <span class="k">del</span> <span class="n">encoder_mask</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_decoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">initial_hidden</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run the model on the source and compute the loss on the target.</span>

<span class="sd">    Args:</span>
<span class="sd">      source: An integer tensor with shape (max_source_sequence_length,</span>
<span class="sd">        batch_size) containing subword indices for the source sentences.</span>
<span class="sd">      target: An integer tensor with shape (max_target_sequence_length,</span>
<span class="sd">        batch_size) containing subword indices for the target sentences.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A scalar float tensor representing cross-entropy loss on the current batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">GetPieceSize</span><span class="p">()</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="c1">#put batch_size first and vocab_size in the middle or this will prevent training</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We define the following functions for training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">model_file</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Train the model and save its best checkpoint.</span>
<span class="sd">  </span>
<span class="sd">  Model performance across epochs is evaluated using token-level accuracy on the</span>
<span class="sd">  validation set. The best checkpoint obtained during training will be stored on</span>
<span class="sd">  disk and loaded back into the model at the end of training.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
  <span class="n">best_accuracy</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">notebook</span><span class="o">.</span><span class="n">trange</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">notebook</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
        <span class="n">make_batch_iterator</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;epoch </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span>
        <span class="n">total</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">))</span> <span class="k">as</span> <span class="n">batch_iterator</span><span class="p">:</span>
      <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch_iterator</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">batch_iterator</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">mean_loss</span><span class="o">=</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">i</span><span class="p">)</span>
      <span class="n">validation_perplexity</span><span class="p">,</span> <span class="n">validation_accuracy</span> <span class="o">=</span> <span class="n">evaluate_next_token</span><span class="p">(</span>
          <span class="n">model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">)</span>
      <span class="n">batch_iterator</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span>
          <span class="n">mean_loss</span><span class="o">=</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">i</span><span class="p">,</span>
          <span class="n">validation_perplexity</span><span class="o">=</span><span class="n">validation_perplexity</span><span class="p">,</span>
          <span class="n">validation_token_accuracy</span><span class="o">=</span><span class="n">validation_accuracy</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">validation_accuracy</span> <span class="o">&gt;</span> <span class="n">best_accuracy</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;Obtained a new best validation accuracy of </span><span class="si">{:.2f}</span><span class="s2">, saving model &quot;</span>
            <span class="s2">&quot;checkpoint to </span><span class="si">{}</span><span class="s2">...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">validation_accuracy</span><span class="p">,</span> <span class="n">model_file</span><span class="p">))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">model_file</span><span class="p">)</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="n">validation_accuracy</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reloading best model checkpoint from </span><span class="si">{}</span><span class="s2">...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_file</span><span class="p">))</span>
  <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">evaluate_next_token</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute token-level perplexity and accuracy metrics.</span>

<span class="sd">  Note that the perplexity here is over subwords, not words.</span>
<span class="sd">  </span>
<span class="sd">  This function is used for validation set evaluation at the end of each epoch</span>
<span class="sd">  and should not be modified.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">total_cross_entropy</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">total_predictions</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">correct_predictions</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">make_batch_iterator</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
      <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
      <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_target</span> <span class="o">=</span> <span class="n">target</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
      <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
          <span class="n">decoder_input</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">)</span>
      <span class="n">total_cross_entropy</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span>
          <span class="n">logits</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">decoder_target</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
          <span class="n">ignore_index</span><span class="o">=</span><span class="n">pad_id</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
      <span class="n">total_predictions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">decoder_target</span> <span class="o">!=</span> <span class="n">pad_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
      <span class="n">correct_predictions</span> <span class="o">+=</span> <span class="p">(</span>
          <span class="p">(</span><span class="n">decoder_target</span> <span class="o">!=</span> <span class="n">pad_id</span><span class="p">)</span> <span class="o">&amp;</span>
          <span class="p">(</span><span class="n">decoder_target</span> <span class="o">==</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
  <span class="n">perplexity</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">total_cross_entropy</span> <span class="o">/</span> <span class="n">total_predictions</span><span class="p">)</span>
  <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct_predictions</span> <span class="o">/</span> <span class="n">total_predictions</span>
  <span class="k">return</span> <span class="n">perplexity</span><span class="p">,</span> <span class="n">accuracy</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now train the baseline model.</p>
<p>Since we haven't yet defined a decoding method to output an entire string, we will measure performance for now by computing perplexity and the accuracy of predicting the next token given a gold prefix of the output.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">baseline_model</span> <span class="o">=</span> <span class="n">Seq2seqBaseline</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;baseline_model.pt&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can load a pre-saved model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">baseline_model</span> <span class="o">=</span> <span class="n">Seq2seqBaseline</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="s1">&#39;baseline_model.pt&#39;</span>
<span class="n">baseline_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">))</span>
<span class="n">p</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">evaluate_next_token</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Perplexity: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">Accuracy: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">a</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Perplexity: 10.29803380933026
Accuracy: 56.91689836268149

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For evaluation, we also need to be able to generate entire strings from the model. We'll first define a greedy inference procedure here. Later on, we'll implement beam search.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict_greedy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make predictions for the given inputs using greedy inference.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        model: A sequence-to-sequence model.</span>
<span class="sd">        sentences: A list of input sentences, represented as strings.</span>
<span class="sd">        max_length: The maximum length at which to truncate outputs in order to</span>
<span class="sd">        avoid non-terminating inference.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        A list of predicted translations, represented as strings.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Once an EOS token has been generated, we force the output</span>
    <span class="c1"># for that example to be padding tokens in all subsequent time steps by</span>
    <span class="c1"># adding a large positive number like 1e9 to the appropriate logits.</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">GetPieceSize</span><span class="p">()</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
    <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">bos_id</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">counter_eos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">cpt</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">counter_eos</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="ow">and</span> <span class="n">cpt</span><span class="o">&lt;</span><span class="n">max_length</span><span class="p">:</span>
        <span class="n">cpt</span><span class="o">+=</span><span class="mi">1</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">counter_eos</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
        <span class="n">counter_eos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">eos_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">counter_eos</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">target</span><span class="p">,</span> <span class="n">pred</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">DecodeIds</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">target</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">target</span>
        
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="s2">&quot;beam&quot;</span><span class="p">}</span>
  <span class="n">source_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">src</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">]</span>
  <span class="n">target_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">trg</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">]</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">start_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">source_sentences</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;greedy&quot;</span><span class="p">:</span>
        <span class="n">prediction_batch</span> <span class="o">=</span> <span class="n">predict_greedy</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">source_sentences</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">prediction_batch</span> <span class="o">=</span> <span class="n">predict_beam</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">source_sentences</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">prediction_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">candidates</span> <span class="ow">in</span> <span class="n">prediction_batch</span><span class="p">]</span>
      <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">prediction_batch</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">sacrebleu</span><span class="o">.</span><span class="n">corpus_bleu</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="p">[</span><span class="n">target_sentences</span><span class="p">])</span><span class="o">.</span><span class="n">score</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Baseline model validation BLEU using greedy search:&quot;</span><span class="p">,</span>
      <span class="n">evaluate</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>

Baseline model validation BLEU using greedy search: 22.390278705269996
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">show_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_examples</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">include_beam</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">validation_data</span><span class="p">[:</span><span class="n">num_examples</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">example</span><span class="o">.</span><span class="n">src</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Target:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">example</span><span class="o">.</span><span class="n">trg</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Greedy prediction:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">predict_greedy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">src</span><span class="p">])[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">include_beam</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Beam predictions:&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">predict_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">src</span><span class="p">])[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">candidate</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Baseline model sample predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">show_predictions</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Baseline model sample predictions:

Input:
  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen
Target:
  A group of men are loading cotton onto a truck
Greedy prediction:
  A group of men are parked on a tree.
Input:
  Ein Mann schläft in einem grünen Raum auf einem Sofa.
Target:
  A man sleeping in a green room on a couch.
Greedy prediction:
  A man is sleeping in a dark room.
Input:
  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.
Target:
  A boy wearing headphones sits on a woman&#39;s shoulders.
Greedy prediction:
  A boy with dreadlocks on his face is sitting down.
Input:
  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf
Target:
  Two men setting up a blue ice fishing hut on an iced over lake
Greedy prediction:
  Two men are in a white house passing a house past a McDonalds.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sequence-to-sequence-model-with-attention">Sequence-to-sequence model with attention<a class="anchor-link" href="#Sequence-to-sequence-model-with-attention">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we extend the baseline model to include an attention mechanism in the decoder. This circumvents the need to store all information about the source sentence in a fixed-size representation, and should substantially improve performance and convergence time.</p>
<p>We use bilinear attention, where the attention distribution over the encoder outputs $e_1, \dots, e_n$ given a decoder LSTM output $d$ is obtained via a softmax of the dot products after a suitable projection: $w_i \propto \exp ( e_i^\top W d )$. The unnormalized attention logits for encoder outputs corresponding to padding tokens should be offset with a large negative value to ensure that the corresponding attention weights are $0$.</p>
<p>After computing the attention distribution, we take a weighted sum of the encoder outputs to obtain the attention context $c = \sum_i w_i e_i$, and add this to the decoder output $d$ to obtain the final representation to be passed to the vocabulary projection layer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Seq2seqAttention</span><span class="p">(</span><span class="n">Seq2seqBaseline</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">,</span> <span class="n">initial_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run the decoder LSTM starting from an initial hidden state.</span>

<span class="sd">    The third and fourth arguments are not used in the baseline model, but are</span>
<span class="sd">    included for compatibility with the attention model in the next section.</span>

<span class="sd">    Args:</span>
<span class="sd">      decoder_input: An integer tensor with shape (max_decoder_sequence_length,</span>
<span class="sd">        batch_size) containing the subword indices for the decoder input. During</span>
<span class="sd">        evaluation, where decoding proceeds one step at a time, the initial</span>
<span class="sd">        dimension should be 1.</span>
<span class="sd">      initial_hidden: A pair of tensors (h_0, c_0) representing the initial</span>
<span class="sd">        state of the decoder, each with shape (num_layers, batch_size,</span>
<span class="sd">        hidden_size).</span>
<span class="sd">      encoder_output: The output of the encoder with shape</span>
<span class="sd">        (max_source_sequence_length, batch_size, 2 * hidden_size).</span>
<span class="sd">      encoder_mask: The output mask from the encoder with shape</span>
<span class="sd">        (max_source_sequence_length, batch_size). Encoder outputs at positions</span>
<span class="sd">        with a True value correspond to padding tokens and should be ignored.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple with three elements:</span>
<span class="sd">        logits: A tensor with shape (max_decoder_sequence_length, batch_size,</span>
<span class="sd">          vocab_size) containing unnormalized scores for the next-word</span>
<span class="sd">          predictions at each position.</span>
<span class="sd">        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as</span>
<span class="sd">          initial_hidden representing the updated decoder state after processing</span>
<span class="sd">          the decoder input.</span>
<span class="sd">        attention_weights: A tensor with shape (max_decoder_sequence_length,</span>
<span class="sd">          batch_size, max_source_sequence_length) representing the normalized</span>
<span class="sd">          attention weights. This should sum to 1 along the last dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">max_source_len</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">max_decoder_len</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_decoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">initial_hidden</span><span class="p">)</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span> <span class="c1">#(source_len, batch_size, 256) do not put batch first</span>
    <span class="n">decoder_output</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#(batch_size, decoder_len, 256)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ilk,imk-&gt;ilm&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">decoder_output</span><span class="p">])</span> <span class="c1">#(batch, max_source_len, max_decoder_len)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">alpha</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoder_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">max_decoder_len</span><span class="p">))</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">max_decoder_len</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(batch, max_decoder_len, max_source_len, 512)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">)</span><span class="o">*</span><span class="n">c</span> <span class="c1">#(batch, max_decoder_len, 512)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_decoder_len</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    <span class="n">decoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">+</span> <span class="n">decoder_output</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">decoder_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">alpha</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">attention_model</span> <span class="o">=</span> <span class="n">Seq2seqAttention</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;attention_model.pt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention model validation BLEU using greedy search:&quot;</span><span class="p">,</span>
      <span class="n">evaluate</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="baf2c255-9fe6-444c-afe8-14f4daf6e9a0"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#baf2c255-9fe6-444c-afe8-14f4daf6e9a0');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "a817203b8463465aaf879cb2f4e33ac2", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="96a8fd9e-661f-4561-bc32-d2752d1a05ac"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#96a8fd9e-661f-4561-bc32-d2752d1a05ac');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "b0ca80fbe6e84bd19e12b27327f7246b", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 53.31, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="6893fd00-0ad1-48b3-8342-8827ae9830d8"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#6893fd00-0ad1-48b3-8342-8827ae9830d8');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "cf11b6739ddd422ab04ad99f1b062383", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 58.94, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="e2097435-90ac-4ce1-8dc2-f95fafabc276"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#e2097435-90ac-4ce1-8dc2-f95fafabc276');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "41778e980a4d4b6a97c5f7c2134b2504", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 61.31, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="8bb7e985-5f86-4f42-9555-4c5f660d7d24"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#8bb7e985-5f86-4f42-9555-4c5f660d7d24');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4da55f40c5744907bb9caff87edaf389", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 62.45, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="0ffc64da-7a65-4e52-a2d0-63ebabd43db8"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#0ffc64da-7a65-4e52-a2d0-63ebabd43db8');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "b6366d63074c4d71b48ebf88a7e5c466", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 63.14, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="afbaaf8a-4d16-4814-aee9-ac5eae3242f5"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#afbaaf8a-4d16-4814-aee9-ac5eae3242f5');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6808da4f641f4ef09e5704e734fb3b98", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 63.94, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="d7d564b6-3054-4812-b53d-5d76171dd5ec"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#d7d564b6-3054-4812-b53d-5d76171dd5ec');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "0f68ef5482a7415aa9b956d79692f937", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Obtained a new best validation accuracy of 65.10, saving model checkpoint to attention_model.pt...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="9b9cb69f-3082-4c70-8551-ad2c209136be"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#9b9cb69f-3082-4c70-8551-ad2c209136be');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4ae799b369fd4a8badb68d4432f9982e", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="b58d8d7b-5556-496b-9e46-90d1eacb1b68"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#b58d8d7b-5556-496b-9e46-90d1eacb1b68');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "e373451e38c4495faa387474117c5c8e", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





 
 
<div id="3297870b-cecf-48ad-9246-7ffb8c9f5d39"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#3297870b-cecf-48ad-9246-7ffb8c9f5d39');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "4b007a6ebba149e1885f0ccde5bf8e6a", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>

Reloading best model checkpoint from attention_model.pt...
Attention model validation BLEU using greedy search: 34.990320881008785
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can load a pre-saved model again:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">attention_model</span> <span class="o">=</span> <span class="n">Seq2seqAttention</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="s1">&#39;attention_model.pt&#39;</span>
<span class="n">attention_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[18]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">evaluate_next_token</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Perplexity: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">Accuracy: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">a</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Perplexity: 6.328194941096404
Accuracy: 65.10349088662342

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention model validation BLEU using greedy search:&quot;</span><span class="p">,</span>
      <span class="n">evaluate</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention model validation BLEU using greedy search: 34.990320881008785
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention model sample predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="n">show_predictions</span><span class="p">(</span><span class="n">attention_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Attention model sample predictions:

Input:
  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen
Target:
  A group of men are loading cotton onto a truck
Greedy prediction:
  A group of men load tree on a truck in a truck.
Input:
  Ein Mann schläft in einem grünen Raum auf einem Sofa.
Target:
  A man sleeping in a green room on a couch.
Greedy prediction:
  A man sleeping in a green room on a couch.
Input:
  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.
Target:
  A boy wearing headphones sits on a woman&#39;s shoulders.
Greedy prediction:
  A boy wearing headphones is sitting on the shoulders of a woman.
Input:
  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf
Target:
  Two men setting up a blue ice fishing hut on an iced over lake
Greedy prediction:
  Two men building a blue ice cream lake on a tree trunk.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Beam-Search">Beam Search<a class="anchor-link" href="#Beam-Search">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now it's time to implement beam search.</p>
<p>Similar to greedy search, beam search generates one token at a time. However, rather than keeping only the single best hypothesis, we instead keep the top $k$ candidates at each time step. This is accomplished by computing the set of next-token extensions for each item on the beam and finding the top $k$ across all candidates according to total log-probability.</p>
<p>Candidates that are finished should stay on the beam through the end of inference. The search process concludes once all $k$ items on the beam are complete.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Node</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">parent</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="o">=</span> <span class="n">parent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1">#if eos tag is present in chain</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">eos</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos</span> <span class="ow">and</span> <span class="n">val</span> <span class="o">==</span> <span class="n">pad_id</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eos</span> <span class="o">==</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">val</span> <span class="o">==</span> <span class="n">eos_id</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eos</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">def</span> <span class="nf">compute_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
         <span class="n">node</span> <span class="o">=</span> <span class="bp">self</span>
         <span class="n">prob</span> <span class="o">=</span> <span class="mf">1.</span>
         <span class="k">while</span> <span class="n">node</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
             <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span><span class="o">*</span><span class="n">node</span><span class="o">.</span><span class="n">prob</span>
             <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parent</span>
         <span class="k">return</span> <span class="n">prob</span>
    <span class="k">def</span> <span class="nf">get_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">node</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">node</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sentence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">parent</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">sentence</span>

<span class="k">class</span> <span class="nc">Batch_Tree</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="p">[</span><span class="n">Node</span><span class="p">(</span><span class="n">bos_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="k">def</span> <span class="nf">first_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">candidates</span><span class="p">):</span> <span class="c1">#(k,vocab_size)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)[:,</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span> <span class="o">=</span> <span class="p">[</span><span class="n">Node</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">],</span> <span class="n">candidates</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]],</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)]</span>
       
    <span class="k">def</span> <span class="nf">add_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">candidates</span><span class="p">):</span> <span class="c1">#(k,vocab_size)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">candidates</span><span class="p">)[:,</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">:]</span> <span class="c1">#keep the k largest values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span> <span class="o">=</span> <span class="p">[[</span><span class="n">Node</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">candidates</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">,</span> <span class="p">[])</span>

    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">leaf</span><span class="o">.</span><span class="n">compute_prob</span><span class="p">()</span> <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">probs</span><span class="p">)[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">get_target</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">val</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">end_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#count eos tokens</span>
        <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">leaf</span><span class="o">.</span><span class="n">eos</span><span class="p">)</span> <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="c1">#true or false</span>
    <span class="k">def</span> <span class="nf">get_prediction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">leaf</span><span class="o">.</span><span class="n">get_sentence</span><span class="p">()</span> <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaves</span><span class="p">]</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="n">sentences</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1">#sort in descending probability order</span>
        <span class="k">return</span> <span class="n">sentences</span>

<span class="k">class</span> <span class="nc">Tree</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">Batch_Tree</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="k">def</span> <span class="nf">first_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">candidates</span><span class="p">):</span> <span class="c1">#(batch_size,k,vocab_size)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">candidates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">first_iteration</span><span class="p">(</span><span class="n">candidates</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
    <span class="k">def</span> <span class="nf">add_candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">candidates</span><span class="p">):</span> <span class="c1">#(batch_size, k, vocab_size)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">candidates</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">add_candidates</span><span class="p">(</span><span class="n">candidates</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">prune</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_target</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">get_target</span><span class="p">()</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">end_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">end_generation</span><span class="p">())</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="c1">#true or false</span>
    <span class="k">def</span> <span class="nf">get_prediction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">tree</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">()</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">predict_beam</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make predictions for the given inputs using beam search.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        model: A sequence-to-sequence model.</span>
<span class="sd">        sentences: A list of input sentences, represented as strings.</span>
<span class="sd">        k: The size of the beam.</span>
<span class="sd">        max_length: The maximum length at which to truncate outputs in order to</span>
<span class="sd">        avoid non-terminating inference.</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">        A list of beam predictions. Each element in the list should be a list of k</span>
<span class="sd">        strings corresponding to the top k predictions for the corresponding input,</span>
<span class="sd">        sorted in descending order by score.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Once an EOS token has been generated, we force the output</span>
    <span class="c1"># for that candidate to be padding tokens in all subsequent time steps by</span>
    <span class="c1"># adding a large positive number like 1e9 to the appropriate logits. This</span>
    <span class="c1"># will ensure that the candidate stays on the beam, as its probability</span>
    <span class="c1"># will be very close to 1 and its score will effectively remain the same as</span>
    <span class="c1"># when it was first completed.  All other (invalid) token continuations will</span>
    <span class="c1"># have extremely low log probability and will not make it onto the beam.</span>

    <span class="c1"># Some special care will need to be taken on the first iteration to ensure that the beam</span>
    <span class="c1"># doesn&#39;t fill up with k identical copies of the same candidate.</span>
    
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">GetPieceSize</span><span class="p">()</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
    <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

    <span class="n">batch_tree</span> <span class="o">=</span> <span class="n">Tree</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch_tree</span><span class="o">.</span><span class="n">get_target</span><span class="p">()</span>
    <span class="n">cpt</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1">#first iteration</span>
    <span class="n">h_0</span> <span class="o">=</span> <span class="n">decoder_hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">*</span><span class="n">k</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c_0</span> <span class="o">=</span> <span class="n">decoder_hidden</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">*</span><span class="n">k</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_0</span><span class="p">,</span> <span class="n">c_0</span><span class="p">)</span>
    
    <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">*</span><span class="n">k</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="mi">256</span><span class="p">)</span>
    <span class="n">encoder_mask</span> <span class="o">=</span> <span class="n">encoder_mask</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">*</span><span class="n">k</span><span class="p">)</span>

    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">[:,:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">*</span><span class="n">k</span><span class="p">),</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="n">batch_tree</span><span class="o">.</span><span class="n">first_iteration</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">batch_tree</span><span class="o">.</span><span class="n">end_generation</span><span class="p">()</span> <span class="o">==</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">cpt</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="n">cpt</span><span class="o">+=</span><span class="mi">1</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch_tree</span><span class="o">.</span><span class="n">get_target</span><span class="p">()</span>

        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">[:,:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">*</span><span class="n">k</span><span class="p">),</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

        <span class="n">batch_tree</span><span class="o">.</span><span class="n">add_candidates</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">batch_tree</span><span class="o">.</span><span class="n">prune</span><span class="p">()</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">batch_tree</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">()</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[[</span><span class="n">vocab</span><span class="o">.</span><span class="n">DecodeIds</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">result</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Baseline model validation BLEU using beam search:&quot;</span><span class="p">,</span>
      <span class="n">evaluate</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;beam&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Baseline model sample predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">show_predictions</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">include_beam</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Baseline model validation BLEU using beam search: 18.307150312400722

Baseline model sample predictions:

Input:
  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen
Target:
  A group of men are loading cotton onto a truck
Greedy prediction:
  A group of men are parked on a tree.
Beam predictions:
  A group of men are parked on a tree.
  A group of men are working on a tree.
  A group of men are parked working on a tree.
  A group of men are parked on a tree stump.
  A group of men are parked together on a tree.

Input:
  Ein Mann schläft in einem grünen Raum auf einem Sofa.
Target:
  A man sleeping in a green room on a couch.
Greedy prediction:
  A man is sleeping in a dark room.
Beam predictions:
  One asleep in a green couch on a couch.
  One asleep in a dark room on a couch.
  One asleep in a dark green couch.
  One asleep in a dark room while sleeping.
  A man is asleep in as he is on the table.

Input:
  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.
Target:
  A boy wearing headphones sits on a woman&#39;s shoulders.
Greedy prediction:
  A boy with dreadlocks on his face is sitting down.
Beam predictions:
  A boy with dreadlocks on his face sits.
  A boy with dreadlocks on his face is sitting down
  A boy with dreadlocks on his face is reading.
  A boy with dreadlocks on his face is sitting on a stage.
  One boy sitting on stage reading for a woman.

Input:
  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf
Target:
  Two men setting up a blue ice fishing hut on an iced over lake
Greedy prediction:
  Two men are in a white house passing a house past a McDonalds.
Beam predictions:
   ⁇  men are putting across across across a boat.
  There are a men are putting across across as as white.
  There are a men are putting across across across a houses a boat.
  There are a men are putting across across across a houses a boat.
  There are a men are putting across across across a houses a house.

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention model validation BLEU using beam search:&quot;</span><span class="p">,</span>
      <span class="n">evaluate</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;beam&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention model sample predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">show_predictions</span><span class="p">(</span><span class="n">attention_model</span><span class="p">,</span> <span class="n">include_beam</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Attention model validation BLEU using beam search: 28.089378643028464

Attention model sample predictions:

Input:
  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen
Target:
  A group of men are loading cotton onto a truck
Greedy prediction:
  A group of men load tree on a truck in a truck.
Beam predictions:
  Group of men loading tree on a trucks.
  A group of men are loading on top of a truck
  A group of men are loading on a truck.
  Group of men loading tree on top of a truck.
  A group of men are loading onto a trucks.

Input:
  Ein Mann schläft in einem grünen Raum auf einem Sofa.
Target:
  A man sleeping in a green room on a couch.
Greedy prediction:
  A man sleeping in a green room on a couch.
Beam predictions:
  Man sleeping man sleeping in a green room..
  Man sleeping man sleeping in a green room..
  There is sleeping in green asleep in a green room.
  There is sleeping in a green room on as room.
  Man sleeping man sleeping in a green room on as.

Input:
  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.
Target:
  A boy wearing headphones sits on a woman&#39;s shoulders.
Greedy prediction:
  A boy wearing headphones is sitting on the shoulders of a woman.
Beam predictions:
  The boy sitting on the shoulders is sitting on the shoulders.
  The boy sitting on a earphones on the shoulders.
  One boy is sitting on a woman..
  One boy is sitting on a woman..
  The boy sitting on the shoulders is sitting on the shoulders of a

Input:
  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf
Target:
  Two men setting up a blue ice fishing hut on an iced over lake
Greedy prediction:
  Two men building a blue ice cream lake on a tree trunk.
Beam predictions:
   ⁇  men building a blue ice lake on a tree trunk.
   ⁇  men building a blue ice lake on a tree trunk lake.
   ⁇  men building a blue ice lake on a tree limb lake lake
   ⁇  men building a blue ice lake on the lake lake lake lake.
   ⁇  men building a blue ice lake on a mountain lake.

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention-visualization">Attention visualization<a class="anchor-link" href="#Attention-visualization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can visualize the decoder attention learned by the attention model on gold source-target pairs from the validation data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">source</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">src</span><span class="p">]</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">trg</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">make_batch</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">attention_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_target</span> <span class="o">=</span> <span class="n">target</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_mask</span><span class="p">)</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Target Sentence&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Source Sentence&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Source: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">src</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Target: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">trg</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
Source: Ein Mann schläft in einem grünen Raum auf einem Sofa.
Target: A man sleeping in a green room on a couch.


Source: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.
Target: A boy wearing headphones sits on a woman&#39;s shoulders.


Source: Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf
Target: Two men setting up a blue ice fishing hut on an iced over lake

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWsAAAEKCAYAAADU7nSHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWsAAAEKCAYAAADU7nSHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0
dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZxddX3/8dc7Catg2BQhQUCMVlzK
Eil1KxW0wVqoVitoK1g0/qpQtVVLa+vW2p9Yi7XWLSpYtIKKilGRRVm0KkiQLQmLMS4krMpWhQeQ
mXf/OGfgMszce2buOXPvmXk/eZzHnOWez/kmGT7zne/5LrJNREQMt3mDLkBERPSWZB0R0QJJ1hER
LZBkHRHRAknWEREtkGQdEdECSdYRETWTdJKkWyStnuS6JP2HpHWSrpS0X6+YSdYREfX7NLCsy/VD
gSXlthz4aK+ASdYRETWz/R3gti4fORw4xYWLgO0k7dIt5oI6C1inBZsvqnVo5T03fLfOcABsteuz
a48ZEVO36b6N6jfG/b9cXznnbP6ovV5LUSMes8L2iik8bhFwfcfxhvLcjZPdMLTJOiJiWJWJeSrJ
uW9J1hERAKMjM/m0jcBuHceLy3OTSpt1RATAyKbqW/9WAq8se4UcCNxpe9ImEEjNOiICAHu0tliS
TgUOAnaStAF4B7BZ8Rx/DDgTeAGwDrgbeFWvmEnWEREAo/Ula9tH9rhu4PVTiZlkHREBUGPNugmN
JWtJv0XRl3BReWojsNL21U09MyJi2mb2BeOUNfKCUdLfAqcBAn5YbgJOlXR8E8+MiOiLR6tvA9BU
zfoY4Mm27+88KelEYA3w3olukrScsqO55i9k3rxHNFS8iIiHcj29PBrTVNe9UWDXCc7vUl6bkO0V
tpfaXppEHREzanS0+jYATdWs3wh8W9KPeXBI5WOBxwPHNvTMiIjpm4svGG2fJekJwAE89AXjJbaH
uxU/IuamIX/B2FhvEBc9zC9qKn5ERK3mYs06IqJ1hvwFY5J1RAQM7MVhVUnWERHAsL9OmzPJuomF
ArKgQcQskjbriIgWSDNIREQLpGYdEdECI/f3/swAJVlHRECaQSIiWiHNIBERLZCadURECwx5sp7x
1c0lTbowpKTlklZJWjU6+puZLFZEzHEeub/yNggznqyBd012IfNZR8TAzMWVYiRdOdklYOcmnhkR
0ZchbwZpqs16Z+APgNvHnRfw/YaeGRExfXO0N8jXgW1sXz7+gqQLGnpmRMT0zcWate1julx7eRPP
jIjoyxytWUdEtMumLD4QETH8UrOevdowR3bmx46oaC62WUdEtE5q1hERLZCadUREC6RmHRHRAukN
EhHRAvagS9BVknVEBAx9m3Vjs+5J+i1JB0vaZtz5ZU09MyJi2kZHq28D0EiylvRXwFeB44DVkg7v
uPwvTTwzIqIvNU6RKmmZpGslrZN0/ATXHyvpfEmXSbpS0gt6xWyqGeQ1wP62fy1pD+B0SXvY/iDF
zHsTkrQcWA6g+QvJnNYRMWNGRmoJI2k+8GHgecAG4BJJK22v7fjYPwBfsP1RSXsDZwJ7dIvbVLKe
Z/vXALZ/JukgioS9O12Ste0VwAqABZsvGu7W/oiYXepr3jgAWGd7PYCk04DDgc5kbeCR5f5C4IZe
QZtqs75Z0j4PlKpI3C8EdgKe2tAzIyKmbwpt1p1LEJbb8o5Ii4DrO443lOc6vRP4M0kbKGrVx/Uq
XlM161cCD+m0aHsT8EpJH2/omRER0zeFQTGdrQDTdCTwadv/Jul3gc9Ieoo9eSGams96Q5dr32vi
mRER/fBobS2vG4HdOo4Xl+c6HQMsA7D9A0lbUrQ83DJZ0EEsmBsRMXzq67p3CbBE0p6SNgeOAFaO
+8wvgIMBJD0J2BK4tVvQDIqJiIDaeoPY3iTpWOBsYD5wku01kt4NrLK9Evgb4BOS3kTxsvFou/sQ
yiTriAiodbCL7TMpXhx2nnt7x/5a4JlTiZlkPWTqXiyg7sUMIAsaxCw15MPNk6wjIiATOUVEtEJq
1hERLVBf171GJFlHREBtvUGakmQdEQE4zSARES0wV5tBJB0A2PYl5RSAy4Bryv6HERHDZS4umCvp
HcChwAJJ5wK/A5wPHC9pX9vvaeK5ERHTNkdr1i8B9gG2AG4CFtu+S9L7gYuBCZN1Fh+IiIHZNDdf
MG6yPQLcLekntu8CsH2PpEl/18jiAxExMHOxGQS4T9LWtu8G9h87KWkhMNx/IxExN83RZpDn2L4X
YNxk2psBRzX0zIiIaZuTXffGEvUE538J/LKJZ0ZE9GWO1qwjItolyToiogUy3DwiYvjVuAZjI5Ks
Z7kmFgqoe0GDLGYQQyHJOiKiBYa8N0jP1c0lbS3pHyV9ojxeIumFzRctImIGjbr6NgA9kzVwMnAv
8Lvl8UbgnxsrUUTEIAx5sq7SDLKX7ZdJOhLA9t2S1HC5IiJmlEeGuxmkSrK+T9JWgAEk7UVR046I
mD1mwQvGdwBnAbtJ+m/gmcDRTRYqImKmtb7rnu1zJf0IOBAQ8IZy2PiUSDrF9iunUcaIiOa1PVlL
ehFwnu1vlMfbSfpj22d0uWfl+FPA70vaDsD2YX2UOSKifsPdZF2tGcT2V8YObN9RrgQzabIGFgNr
gU9StHULWAr8W7cHZfGBiBgUbxrubF2l695En+mV5JcClwJvA+60fQFwj+0LbV842U22V9heantp
EnVEzKjRKWwDUKVmvUrSicCHy+PXUyTiSZVzWH9A0hfLrzdXfFZExEC0/gUjcBzwj8Dny+NzKRJ2
T7Y3AC+V9IfAXdMqYUTETBjuVpBKvUF+Axzfz0PKl5Pf6CdGRESTWl+zlvQE4M3AHp2ft/3c5ooV
ETHD2l6zBr4IfIyiZ8dwz84dETFN3jToEnRXJVlvsv3RxksSrZE5smM28pDXrKt03fuapNdJ2kXS
DmNb4yWLOaPuRB0xLTV23ZO0TNK1ktZJmvCdn6Q/lbRW0hpJn+sVs0rN+qjy61s6zhl4XIV7IyJa
oa6ataT5FF2dnwdsAC6RtNL22o7PLAH+Dnim7dslPbpX3Cq9QfacfrEjItqhxmaQA4B1ttcDSDoN
OJxiVPeY1wAftn07gO1begWtulLMP0haUR5npZiImHU8osqbpOWSVnVsyztCLQKu7zjeUJ7r9ATg
CZK+J+kiSct6la9KM8jJFCMWn1Eeb6ToIfL1CvdGRLTCVGrWtlcAK/p43AJgCXAQxVxK35H0VNt3
THZDlReMe9l+H3B/Wci7KSZmioiYNTyqylsPG4HdOo4Xl+c6bQBW2r7f9k+B6yiS96SqJOusFBMR
s55Hq289XAIskbSnpM2BI4Dx00afQVGrRtJOFM0i67sFrdIM8k4evlLMqyrc9wBJz6JodF9t+5yp
3BsRMRPsehoMbG+SdCxwNjAfOMn2GknvBlbZXllee76ktRSDDd9i+1fd4sruPR5e0o48uFLMRb1W
ipH0Q9sHlPuvoZj46SvA84Gv2X7vJPd1zme9f6ZJnRua6GedQTFzy6b7NvadaTf8znMrTw6y+OLz
ZrwpuMrcIN+2fTAdEzF1nJvMZh37y4Hn2b5V0vuBi4AJk3Vno/2CzRcN96wqETGrjI4M96u4SZO1
pC2BrYGdJG3Pgy8VH8nDu6GMN6+8Zx5F7f1WKGbwkzTkI/AjYi6q8OJwoLrVrF8LvBHYlaLr3tif
5C7gP3vEXdhxjyXtYvtGSduQniQRMYRam6xtfxD4oKTjbH9oKkFt7zHJpVHgRVOJFRExEyq8vhuo
KsPNPyTpGTx8PutTpvqwso/2T6d6X0RE01pbsx4j6TPAXsDlPDiftYEpJ+uIiGFVV9e9plTpZ70U
2NtV+vhFRLTUSFt7g3RYDTwGuLHhssQc1YbFDCB9t2e72VCz3glYK+mHdAwzt31YY6WKiJhhrW+z
phhuHhExqw17Q2+V3iAXStodWGL7W5K2phjvHhExa7S+Zl3O7bEc2IGiV8giitXOuw03j4holZHR
KpOQDk6V0r2eYqa9uwBs/xjouV5YRESb2NW3QajSZn2v7fuk4lcESQso57aOiJgtRoe8N0iVmvWF
kv4e2ErS8yiW9Ppatxsk/Y6kR5b7W0l6l6SvSTpB0sL+ix0RUS9blbdBqJKsjwduBa6imNzpTOAf
etxzEnB3uf9BiomdTijPnTytkkZENKj1zSC2R4FPSPov4MnAxgqjGefZHpsKdant/cr9/5F0+WQ3
jVt8gCw+EBEzpbXNIJI+JunJ5f5CirlBTgEuk3Rkj7irJY0t/XWFpKVlnCdQLrw7EdsrbC+1vTSJ
OiJm0sjovMrbIHR76rNtryn3XwVcZ/upwP7AW3vEfTXwe5J+AuwN/EDSeuAT5bWIiKHiKWyD0K0Z
5L6O/bEXi9i+aaxnyGRs3wkcXb5k3LN8zgbbN/dX3IiIZgx7M0i3ZH2HpBcCGyn6WR8DD3Td26pK
cNt3AVf0W8iIiKa1eSKn1wL/QTHj3htt31Sef8jiuRERs8HooAvQQ7dlva4Dlk1w/mzg7CYLFREx
0zzky8NWGcEYETHrbWpxM0hEa83VhQLqXnRhLv09pmYdEdECw95m3bN3t6SdJX1K0jfL470lHdN8
0SIiZo5R5W0QqgzF+TTFC8Vdy+PrgDc2VaCIiEEYncI2CFWS9U62v0BZxnLOj5FGSxURMcNGUOVt
EKq0Wf9G0o6UoywlHQjc2WipIiJm2JCv6lUpWf81sBLYS9L3gEcBL2m0VBERM2y07b1BbP9I0u8B
TwQEXGt70pnzACT9FfAV29fXU8yIiGYN+/JXVXqDvB7YxvYa26uBbSS9rsdt/wRcLOm7kl4n6VF1
FDYioimz4QXja2zfMXZg+3bgNT3uWQ8spkja+wNrJZ0l6ShJ2052k6TlklZJWjU6+psKRYuIqMeo
VHkbhCrJer465kSVNB/YvMc9tj1q+xzbx1B0+/sIxVwj67vclMUHImIgRqawDUKVZH028HlJB0s6
GDgVOKvHPQ/50WP7ftsrbR8J7D69okZENGdU1bdeJC2TdK2kdZKO7/K5P5HksdW0uqnSG+StFOsi
/mV5fC7wyR73vGyyC7bvnuxaRMSg1NUbpGx9+DDFoi0bgEskrbS9dtzntgXeAFxcJW7XZF0+9BTb
rwA+VrWw5fSqERGtUWNvkAOAdbbXA0g6DTgcWDvuc/8EnAC8pUrQrs0gtkeA3SX1aqOOiGi1qTSD
dHaGKLflHaEWAZ3dljeU5x4gaT9gN9uVF3Kp0gyyHviepJXAA100bJ9Y9SEREcNuKl3ybK8AVkzn
OZLmAScCR0/lvirJ+iflNg+YtNtdRESbjdTXI28jsFvH8eLy3JhtgacAF5Qd7R4DrJR0mO1VkwWt
MoLxXdMqbkTMuLoXC6h7MQMY3gUNahzscgmwRNKeFEn6CODlYxdt3wnsNHYs6QLgzd0SNVRI1pLO
Z4K2d9vPrVryiIhhV1eytr1J0rEU3Z7nAyfZXiPp3cAq2yunE7dKM8ibO/a3BP4E2DSdh0VEDKs6
l2C0fSZw5rhzb5/kswdViVmlGeTScae+J+mHVYJHRLTFsC/rVaUZZIeOw3kUc30sbKxEEREDMOwr
qlRpBrmUos1aFM0fPwWyBmNEzCqtX3zA9p4zUZCIiEGaDc0gm1HMC/Kc8tQFwMe7LUBQjng8ArjB
9rckvRx4BnA1sKLX4gURETOt9cka+CiwGcUUpwB/Xp57dZd7Ti5jby3pKGAb4MvAwRTj5o+aboEj
Ipow7CvFVEnWT7f92x3H50m6osc9T7X9NEkLKDqF72p7RNJngUnvLcfXLwfQ/IVkTuuImCnD3mZd
ZT7rEUl7jR1Iehy9X5zOK5tCtgW25sHeI1tQ1NInlMUHImJQhn3xgSo167cA50taT9EjZHfgVT3u
+RRwDcXonbcBXyzvPxA4bfrFjYhoxuiQN4RU6Q3ybUlLKFY3h2J183t73PMBSZ8v92+QdApwCPAJ
2xlQExFDp7UvGCU9Hbje9k2275W0D8VQ859Leqft27oFtn1Dx/4dwOl1FToiom7DXa/u3mb9ceA+
AEnPAd4LnALcyTTncY2IGFajU9gGoVszyPyO2vPLKPpHfwn4kqTLmy9aRMTM2aThrlt3TdaSFtje
RNE/unPZmiovJiOiizbMFT2sc083YbhTdfekeypwoaRfAvcA3wWQ9HiKppCIiFmjtS8Ybb9H0reB
XYBzbI/94JkHHDcThYuImCmt7rpn+6IJzl3XXHEiIgZjuFN12p4jIoAWN4NERMwlI0Net06yjohg
DtesywmfXgzsRjH3yXXA52zf1dQzIyKmy0Nes64y696USfor4GMUq6E/nWK2vd2AiyQd1MQzIyL6
0eYRjP14DbBPOYf1icCZtg+S9HHgq8C+E92U+awjYlCGveteIzXr0tgPgi0oVorB9i/IfNYRMYQ8
hW0QmqpZfxK4RNLFwLOBEwAkPQroOltfRMQgbBrymnUjydr2ByV9C3gS8G+2rynP38qDC+9GRAyN
YX/B2FhvENtrgDVNxY+IqNOc7boXEdEmc7ZmHRHRJqlZR0S0wIhTs46YFepeLGAuTezfBsPezzrJ
OiKCtFlHRLRC2qwjIlpg2JtBmhxuHhHRGp7Cf71IWibpWknrJB0/wfW/lrRW0pWSvi1p914xk6wj
Iih6g1TdupE0H/gwcCiwN3CkpL3HfewyYKntpwGnA+/rVb4k64gIimaQqlsPBwDrbK+3fR9wGnB4
5wdsn2/77vLwImBxr6BJ1hERTG0+a0nLJa3q2JZ3hFoEXN9xvKE8N5ljgG/2Kl9eMEZEMLWue7ZX
ACv6faakPwOWAr/X67NNrRSzUNJ7JV0j6TZJv5J0dXluuy73PfDTanT0N00ULSJiQjU2g2ykWBlr
zOLy3ENIOgR4G3CY7Xt7BW2qGeQLwO3AQbZ3sL0j8PvluS9MdlMWH4iIQbFdeevhEmCJpD0lbQ4c
Aazs/ICkfYGPUyTqW6qUr6lkvYftE2zfNHbC9k22TwB6dlGJiJhpI7jy1o3tTcCxwNnA1cAXbK+R
9G5Jh5Uf+1eKFbS+KOlySSsnCfeAptqsfy7prcB/2b4ZQNLOwNE8tOE9ImIo1DkoxvaZwJnjzr29
Y/+QqcZsqmb9MmBH4MKyzfo24AJgB+ClDT0zImLaamwGaURTy3rdDvxtuT2EpFcBJzfx3IiI6cpw
84d71wCeGRHRVZ3DzZvQSM1a0pWTXQJ2buKZERH9mKuLD+wM/AFFV71OAr7f0DMjGlX3YgF/+Jh9
a40H8I2bLqs13v9+5GW1xgPY9nWfrz1mHYa9GaSpZP11YBvbl4+/IOmChp4ZETFtczJZ2z6my7WX
N/HMiIh+DKqXR1WZGyQigjlas46IaJuswRgR0QIjHu5VGJOsIyJIm3VERCukzToiogWGvc16xoeb
S5p0+ZosPhARgzJqV94Goanh5vtNdgnYZ7L7OpfKWbD5ouH+MRcRs8qw16ybaga5BLiQIjmPN+my
XhERgzJXe4NcDbzW9o/HX5CUxQciYugMqnmjqqaS9TuZvD38uIaeGRExbXOyGcT26V0ub9/EMyMi
+jHsNessPhARQRYfeNglsvhARAyhEY8MughdqYkhlpJupsviA7Z37RUjXfciZqd7bvhu7TE32+lx
E/U8m5LH7vDUyjnnF7dd1ffzpiqLD0REMEeHm2fxgYhom0zkFBHRAsPeGyTJOiKCOdrPOiKibebq
cPOIiFZJm3VERAukzXoKJC0HlgNo/kLmzXvEgEsUEXPFsNesGxluLumRkv6/pM9Ievm4ax+Z7D7b
K2wvtb00iToiZtIorrwNQlNzg5xMMVrxS8ARkr4kaYvy2oENPTMiYtpsV94GoalmkL1s/0m5f4ak
twHnSTqsoedFRPRlrvYG2ULSPLv409t+j6SNwHeAbRp6ZkTEtA37C8ammkG+Bjy384TtTwN/A9zX
0DMjIqZt2JtBGknWtt9q+1sTnD8L+JcmnhkR0Y8657OWtEzStZLWSTp+gutbSPp8ef1iSXv0ipnF
ByIiqK9mLWk+8GHgUGBv4EhJe4/72DHA7bYfD3wAOKFX+bL4QEQEtbZZHwCss70eQNJpwOHA2o7P
HE6xVi3A6cB/SpK7/SSYyk+TKfzUuRnYB9h93LYHcEMDz1s+zPHmahnn6p+7DWWcy3/uusoFrOrY
lndcewnwyY7jPwf+c9z9q4HFHcc/AXbq9symmkHGFh/4+bjtZ8AFDTxv+ZDHayJmG8rYRMyUcXhj
tqGMtXDHAL5yW9H0M7P4QEREvTYCu3UcLy7PTfSZDZIWAAuBX3ULOogXjBERs9klwBJJe0raHDgC
WDnuMyuBo8r9lwDnuWwPmcxQTeTUh7p/BWniV5q5WMYmYqaMwxuzDWVsnO1Nko4FzgbmAyfZXiPp
3cAq2yuBTwGfkbQOuI0ioXfVyOrmERFRrzSDRES0QJJ1REQLtDpZ9xrSOY14J0m6RdLqmsq3m6Tz
Ja2VtEbSG2qIuaWkH0q6ooxZy4hQSfMlXSbp6zXF+5mkqyRdLmlVTTG3k3S6pGskXS3pd/uI9cSy
bGPbXZLeWEMZ31T+u6yWdKqkLfuM94Yy1prplm+i72tJO0g6V9KPy6/b1xDzpWU5RyUtrSHev5b/
1ldK+oqk7aYSc9YZdOfyPjqlz6foSP44YHPgCmDvPmM+B9gPWF1TGXcB9iv3twWuq6GMoujDDrAZ
cDFwYA1l/Wvgc8DXa/qz/4wenfynEfO/gFeX+5sD29X4vXQTsHufcRYBPwW2Ko+/ABzdR7ynUAye
2JqiM8C3gMdPI87Dvq+B9wHHl/vHAyfUEPNJwBMpxlIsrSHe84EF5f4JUy3jbNvaXLN+YEin7fuA
sSGd02b7OxRvZmth+0bbPyr3/xe4muJ/6H5i2vavy8PNyq2vt8SSFgN/CHyynzhNkrSQ4n/oTwHY
vs/2HTWFPxj4ie2f1xBrAbBV2Xd2a+CGPmI9CbjY9t22NwEXAi+eapBJvq8Pp/jhR/n1j/uNaftq
29dOtXxd4p1T/rkBLqLorzxntTlZLwKu7zjeQJ+JsEnlrFr7UtSE+401X9LlwC3Aubb7jfnvwFuB
OmdfN3COpEvLtTX7tSdwK3By2VzzSUl1rf12BHBqv0FsbwTeD/wCuBG40/Y5fYRcDTxb0o6StgZe
wEMHW/RjZ9s3lvs3Mfxz9vwF8M1BF2KQ2pysW0PSNhRLnL3R9l39xrM9YnsfiprGAZKe0kfZXgjc
YvvSfss1zrNs70cx89jrJT2nz3gLKH5N/qjtfYHfUPz63pdy0MJhwBdriLU9RY11T2BX4BGS/my6
8WxfTfHr/znAWcDlwEi/5ZzgOabP386aVK40tQn470GXZZDanKyrDOkcOEmbUSTq/7b95Tpjl80A
5wPL+gjzTOAwST+jaEp6rqTP1lC2jeXXW4CvUDRb9WMDsKHjt4jTKZJ3vw4FfmT75hpiHQL81Pat
tu8Hvgw8o5+Atj9le3/bzwFup3jvUYebJe0CUH69paa4tZJ0NPBC4BXlD5U5q83JusqQzoGSJIo2
1qttn1hTzEeNvRWXtBXwPOCa6caz/Xe2F9veg+Lv8Dzb064NluV6hKRtx/YpXhT11cPG9k3A9ZKe
WJ46mIdOOTldR1JDE0jpF8CBkrYu/+0PpnhPMW2SHl1+fSxFe/Xn+i5loXO481HAV2uKWxtJyyia
5w6zffegyzNwg37D2c9G0YZ3HUWvkLfVEO9UirbG+ylqcsf0Ge9ZFL9eXknxK+zlwAv6jPk04LIy
5mrg7TX+fR5EDb1BKHroXFFua+r4tynj7kMxHeWVwBnA9n3GewTF5DkLa/w7fBfFD8/VwGeALfqM
912KH0pXAAdPM8bDvq+BHYFvAz+m6GWyQw0xX1Tu30sxTfLZfcZbR/Feauz/nY/V9e/Uxi3DzSMi
WqDNzSAREXNGknVERAskWUdEtECSdURECyRZR0S0wGxZKSZqIGmsOxfAYyhGy91aHh/gYg6Wup+5
H/Bo22dNcG0bivlKnkwxgdXtwB94Gn1uJb0YWGt72n3SIwYpyToeYPtXFH2ZkfRO4Ne231/1fknz
bU91OPR+FLPLPSxZA28CfmH7iDL+b1H0w52OF1PMfZJkHa2UZpCoRNLXykmZ1kh6dXlugaQ7JP27
pCsp5ik5rJxj/FJJH5J0RvnZbSR9upyL+zJJf1SOwHw78IpyTumXjHvsLnRMIWD7GhfDuJF0VBnr
ckkfkTSvozzvVTHf9w8kPVrSsykGUH2g/PwekpZIOrss53ckPaGM+1lJH5T0fUnrJb2o4+/g71XM
0X2FpPeU5yaME1G7QY/KyTacG/BO4M0dxzuUX7emGFG3PcVvZgZe3HFtA7A7RbPFF4EzymvvA44o
97enGHm6JfBq4N8nKcP+FM0w3wf+iXIuZ4qa+Bk8ONfxCuDlHeU5tDx/Ig/O2fxZ4I87Yp8P7FXu
PxM4p+Nzp5blfxpwTXn+jyhGE2417u9jwjjZstW9pRkkqnqTpMPK/cXAXhRDgO+jmKgJYG/gWpfz
Qks6FXhlee35wKF6cEWfLYHHdnug7UslPa689xBglaQDyv2nl8cAW/HgdLn32B6bSvNS4Nnj45Zz
qxwIfKm8Hx7aJHiGbQNXShqbdvcQilWq7ynLdluFOBG1yTdW9CTpEIqJ/w+0fY+k/6FItlAkxypz
FoiiZvuTcbG7Tp3qYtGGL1EkRFHMkieKxPmP42ItoPjhMWaEib/HBfzSxTSzE7l33Gcn0ytORG3S
Zh1VLARuKxP1kylqtRNZCzxRxdqTAl7Wce1s4LixA0n7lrv/S7Hk2cNIelbHDINbUKyc8nOKiYf+
VNJO5bUdy1npunngObZvB24ca48u27t/u8f95wJ/UbazI2mHacaJmJYk66jiG8DWktYC/8wkq924
6FJ3LEUyXQXcAdxZXn4XxWT8V0laQ9EmDnAe8NvlS8fxLxiXAN+VdBXwI+AHwFdtX1XG+1b5YvMc
eq90cirw92MvGCmmg/1/ksZmBnxht5ttf52ix8oqFav0vKm8NKU4EdOVWfeiVpK2sf3rsmb9ceAq
2x8adLki2i4166jbX5Y1z4HSRWgAAAAySURBVLUUL/4+MeDyRMwKqVlHRLRAatYRES2QZB0R0QJJ
1hERLZBkHRHRAknWEREt8H+n97C6DkRbIwAAAABJRU5ErkJggg==
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWsAAAENCAYAAADJ60Q/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generally, it seems that the attention gives all of the weight to encoder and decoder hidden states that have approximately the same place in the sentence.This results in an almost identity-like attention matrix. 
<br><br>In some cases however, we can see the weight beginning to spread further from the diagonal, implying that certain dependencies between words arise in different orders. 
<br><br>
For example, the end of the second source sentence is: "den Schultern einer Frau" which correspond to the target "a woman's shoulders". As the genitive takes a different form in english and in german, we can see the attention mechanism is attributing weights to words in a different order than the identity matrix.
<br><br>
Additionally, some words and adjectives can be combined into one in german (Eisfischerhütte -&gt; ice fishing hut), which results in zero entries in the attention weights. There is also syntactic dependencies at the end of the sentence that require looking at the words in a different order.
<br><br>
Mostly, the observed differences come from syntactic differences between both languages.</p>

</div>
</div>
</div>
    </div>
  </div>
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        
        </body>
</html>
<script src='../scroll-fade.js'></script>
<!-- scripts particles-->
<script src="../js/particles.js"></script>
<script src="../js/app.js"></script>
