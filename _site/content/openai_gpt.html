<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Improving Language Understanding by Generative Pre-Training (GPT) | Timothy Delille</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Improving Language Understanding by Generative Pre-Training (GPT)" />
<meta name="author" content="OpenAI, 2018" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I’m a data scientist working at PwC AI Labs with a passion for jazz piano and martial arts. I hold a Masters in Computer Science from UC Berkeley. Trying to uncover the truth in our world without creating unnecessary entropy. “What is not brought to consciousness comes to us as fate.”" />
<meta property="og:description" content="I’m a data scientist working at PwC AI Labs with a passion for jazz piano and martial arts. I hold a Masters in Computer Science from UC Berkeley. Trying to uncover the truth in our world without creating unnecessary entropy. “What is not brought to consciousness comes to us as fate.”" />
<link rel="canonical" href="https://timothydelille.github.io/content/openai_gpt.html" />
<meta property="og:url" content="https://timothydelille.github.io/content/openai_gpt.html" />
<meta property="og:site_name" content="Timothy Delille" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-13T15:59:23-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Improving Language Understanding by Generative Pre-Training (GPT)" />
<script type="application/ld+json">
{"url":"https://timothydelille.github.io/content/openai_gpt.html","headline":"Improving Language Understanding by Generative Pre-Training (GPT)","dateModified":"2021-06-13T15:59:23-04:00","datePublished":"2021-06-13T15:59:23-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://timothydelille.github.io/content/openai_gpt.html"},"author":{"@type":"Person","name":"OpenAI, 2018"},"description":"I’m a data scientist working at PwC AI Labs with a passion for jazz piano and martial arts. I hold a Masters in Computer Science from UC Berkeley. Trying to uncover the truth in our world without creating unnecessary entropy. “What is not brought to consciousness comes to us as fate.”","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<!--<link rel="stylesheet" href="/assets/main.css">--><link type="application/atom+xml" rel="alternate" href="https://timothydelille.github.io/feed.xml" title="Timothy Delille" /><!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
  <!-- Fontawesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.2/css/all.css" integrity="sha384-/rXc/GQVaYpyDdyxK+ecHPVYJSN9bmVFBvjA/9eOB+pb3F2w2N6fc5qB9Ew5yIns" crossorigin="anonymous">

  <!-- handle markdown images -->
  <style>
    img {
      max-width: 100%;
    }
  </style>
</head>
<body><nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
  <div class="container-fluid">
    <!--<img src="../assets/img/profile_pic.jpg" alt="" class="d-inline-block align-text-center rounded" width="30">-->
    <a class="navbar-brand" href="/">Timothy Delille</a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
      <ul class="navbar-nav"><li class="nav-item">
            <a class="nav-link" aria-current="page" href="/about/">About</a>
          </li></ul>
    </div>
  </div>
</nav><!--<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <p class="display-5" itemprop="name headline">Improving Language Understanding by Generative Pre-Training (GPT)</p><p class="text-muted">OpenAI, 2018</p></p>

<ul id="markdown-toc">
  <li><a href="#1-generative-pre-training" id="markdown-toc-1-generative-pre-training">1. Generative pre-training</a></li>
  <li><a href="#2-discriminative-fine-tuning" id="markdown-toc-2-discriminative-fine-tuning">2. Discriminative fine-tuning</a></li>
  <li><a href="#architecture" id="markdown-toc-architecture">Architecture</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
  <li><a href="#notes" id="markdown-toc-notes">Notes</a></li>
</ul>

<h2 id="1-generative-pre-training">1. Generative pre-training</h2>
<p>Language modeling objective on unlabeled data using auto-regressive model:</p>

\[L_1(\{u_1, \dots, u_n\})=\sum_i \log P(u_i\vert u_{i-k}, \dots, u_{i-1}; \Theta)\]

<p>where \(k\) is the size of the context window and \(u_i\)s the tokens in the corpus.</p>

<p><em>BooksCorpus</em> dataset is used for training (7,000 unique unpublished books from a variety of genres). It contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information.</p>

<p>The <em>1B Word Benchmark</em> used by ELMo is approximately the same size but is shuffled at a sentence level - destroying long-range structure.</p>

<h2 id="2-discriminative-fine-tuning">2. Discriminative fine-tuning</h2>
<p>Task-specific input adaptation and corresponding supervised objective</p>

\[L_2(\mathcal{D}) = \sum_{(x,y)} \log P(y\vert x^1, \dots, x^m)\]

<p>where \(P(y\vert x^1,\dots, x^m) = \text{softmax}(h_l^m W_y)\). \(h_l^m\) is the final transformer block’s activation and \(W_y\) is a task-specific parameter learned during fine-tuning.</p>

<p>Including language modeling as <strong>auxiliary objective</strong> to the fine-tuning improves generalization: \(L_3 = L_2 + \lambda L_1\)</p>

<p>Pre-trained model is trained on contiguous sequences of text, thus inputs for fine-tuning tasks need to be adapted to a <em>traversal-style</em> approach:</p>

<p><img src="../assets/img/openai_gpt_input_transformations.png" alt="input transformations" /></p>

<p>Embeddings for delimiter tokens are parameters that arise during fine-tuning.</p>

<h2 id="architecture">Architecture</h2>
<ul>
  <li>multi-layer <em>Transformer decoder</em></li>
  <li>provides structured memory for handling long-term dependencies than attention-augmented RNNs.</li>
</ul>

<h2 id="results">Results</h2>
<p>(improvements are absolute)</p>
<ul>
  <li>86.5 / +8.9% on commonsense reasoning (Stories Cloze Test)</li>
  <li>59 / +5.7% on question answering (RACE)</li>
  <li>81.4 / +1.5% on textual entailment (MultiNLI) (judge relationship as entailment, contradiction or neutral)</li>
  <li>72.8 / +5.5% on GLUE multi-task benchmark</li>
</ul>

<p>Larger fine-tuning datasets benefit from the language model auxiliary objective but smaller datasets do not.</p>

<p>Transformers beats LSTM-based architectures on almost all datasets.</p>

<h2 id="notes">Notes</h2>
<ul>
  <li>Zero-shot behavior: perform task without supervised fine-tuning</li>
  <li>earliest approaches used unlabeled data to compute word-level or phrase-level statistics, then used as a feature in a supervised model before adopting to word embeddings</li>
  <li>used <em>ftfy</em> library to fix unicode that’s broken and <em>spaCy</em> tokenizer</li>
  <li>Mathews correlation coefficient: measure of the quality of binary classification. Computed using confusion matrix, regarded as balanced measure which can be used even in the case of class imbalance (better than F1 score). See <a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">wikipedia</a>.</li>
</ul>
<a class="u-url" href="/content/openai_gpt.html" hidden></a>
      </div>
    </main>--><div class='container-fluid bg-transparent'>
      <div class='row'>
        <div class="col-lg-6 offset-lg-3 col-sm-8 offset-sm-2 col-xs-12 bg-body border-start border-end p-5">
          <p class="display-5" itemprop="name headline">Improving Language Understanding by Generative Pre-Training (GPT)</p><p class="text-muted">OpenAI, 2018</p></p>

<ul id="markdown-toc">
  <li><a href="#1-generative-pre-training" id="markdown-toc-1-generative-pre-training">1. Generative pre-training</a></li>
  <li><a href="#2-discriminative-fine-tuning" id="markdown-toc-2-discriminative-fine-tuning">2. Discriminative fine-tuning</a></li>
  <li><a href="#architecture" id="markdown-toc-architecture">Architecture</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
  <li><a href="#notes" id="markdown-toc-notes">Notes</a></li>
</ul>

<h2 id="1-generative-pre-training">1. Generative pre-training</h2>
<p>Language modeling objective on unlabeled data using auto-regressive model:</p>

\[L_1(\{u_1, \dots, u_n\})=\sum_i \log P(u_i\vert u_{i-k}, \dots, u_{i-1}; \Theta)\]

<p>where \(k\) is the size of the context window and \(u_i\)s the tokens in the corpus.</p>

<p><em>BooksCorpus</em> dataset is used for training (7,000 unique unpublished books from a variety of genres). It contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information.</p>

<p>The <em>1B Word Benchmark</em> used by ELMo is approximately the same size but is shuffled at a sentence level - destroying long-range structure.</p>

<h2 id="2-discriminative-fine-tuning">2. Discriminative fine-tuning</h2>
<p>Task-specific input adaptation and corresponding supervised objective</p>

\[L_2(\mathcal{D}) = \sum_{(x,y)} \log P(y\vert x^1, \dots, x^m)\]

<p>where \(P(y\vert x^1,\dots, x^m) = \text{softmax}(h_l^m W_y)\). \(h_l^m\) is the final transformer block’s activation and \(W_y\) is a task-specific parameter learned during fine-tuning.</p>

<p>Including language modeling as <strong>auxiliary objective</strong> to the fine-tuning improves generalization: \(L_3 = L_2 + \lambda L_1\)</p>

<p>Pre-trained model is trained on contiguous sequences of text, thus inputs for fine-tuning tasks need to be adapted to a <em>traversal-style</em> approach:</p>

<p><img src="../assets/img/openai_gpt_input_transformations.png" alt="input transformations" /></p>

<p>Embeddings for delimiter tokens are parameters that arise during fine-tuning.</p>

<h2 id="architecture">Architecture</h2>
<ul>
  <li>multi-layer <em>Transformer decoder</em></li>
  <li>provides structured memory for handling long-term dependencies than attention-augmented RNNs.</li>
</ul>

<h2 id="results">Results</h2>
<p>(improvements are absolute)</p>
<ul>
  <li>86.5 / +8.9% on commonsense reasoning (Stories Cloze Test)</li>
  <li>59 / +5.7% on question answering (RACE)</li>
  <li>81.4 / +1.5% on textual entailment (MultiNLI) (judge relationship as entailment, contradiction or neutral)</li>
  <li>72.8 / +5.5% on GLUE multi-task benchmark</li>
</ul>

<p>Larger fine-tuning datasets benefit from the language model auxiliary objective but smaller datasets do not.</p>

<p>Transformers beats LSTM-based architectures on almost all datasets.</p>

<h2 id="notes">Notes</h2>
<ul>
  <li>Zero-shot behavior: perform task without supervised fine-tuning</li>
  <li>earliest approaches used unlabeled data to compute word-level or phrase-level statistics, then used as a feature in a supervised model before adopting to word embeddings</li>
  <li>used <em>ftfy</em> library to fix unicode that’s broken and <em>spaCy</em> tokenizer</li>
  <li>Mathews correlation coefficient: measure of the quality of binary classification. Computed using confusion matrix, regarded as balanced measure which can be used even in the case of class imbalance (better than F1 score). See <a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">wikipedia</a>.</li>
</ul>
<a class="u-url" href="/content/openai_gpt.html" hidden></a>
        </div>
      </div>
    </div><!--<footer class="site-footer h-card">-->
  <!--<data class="u-url" href="/"></data>-->

  <div class="container-fluid p-3 bg-light border-top">
    <div class="row">
      <div class="col-4">
        <p class="lead">Timothy Delille</p>
      </div>
      <div class="col-4">
        <!--<a class="u-email" href="mailto:timothydelille at aol dot com">timothydelille at aol dot com</a>-->
        <p>timothydelille at aol dot com</p>
        <br>
        <a href='https://github.com/TimothyDelille' style="text-decoration:None;">
          <i class="fab fa-github"></i> timothydelille
        </a>
        <br>
        <a href='https://linkedin.com/in/timothydelille' style="text-decoration:None;">
          <i class="fab fa-linkedin"></i> timothydelille
        </a>
      </div>

    <div class="col-4">
      <p class="text-justify">I&#39;m a data scientist working at PwC AI Labs with a passion for jazz piano and martial arts.  I hold a Masters in Computer Science from UC Berkeley.  Trying to uncover the truth in our world without creating unnecessary entropy.  &quot;What is not brought to consciousness comes to us as fate.&quot;</p>
    </div>

    </div>

  </div>

<!--</footer>-->
<!-- Mathjax -->
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

    <!-- scripts particles-->
    <script src="../particles/particles.js"></script>
    <script src="../particles/app.js"></script>
  </body>

</html>

