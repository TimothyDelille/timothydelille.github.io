<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <!-- Fontawesome -->
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.2/css/all.css" integrity="sha384-/rXc/GQVaYpyDdyxK+ecHPVYJSN9bmVFBvjA/9eOB+pb3F2w2N6fc5qB9Ew5yIns" crossorigin="anonymous">
        
        <!-- Google Fonts -->
        <link href="https://fonts.googleapis.com/css?family=Lexend+Deca|Noto+Sans:600|Work+Sans:400,500|Playfair+Display+SC:700&display=swap" rel="stylesheet">
    
        <!-- Bootstrap CSS -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

        <!-- Mathjax -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <!-- CSS sheets -->
        <link rel="stylesheet" href="../style/index.css?v=<?=time();?>" type="text/css"/>
		<title>Timothy Delille</title>
	</head>
    <body class='bg-light'>
        <nav class="navbar navbar-expand-lg navbar-light scroll-fade">
          <img class='mx-auto' src='../Delille_Timothy.jpg' style='width:3rem;border-radius:50%;'/>
          <h3 class='ml-2'><a href='../index.html'>Timothy Delille</a></h3>
          <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav ml-auto">
              <li class="nav-item active">
                <a class="nav-link" href="../index.html">Work <span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="#">About</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="#">Resume</a>
              </li>
            </ul>
          </div>
        </nav>
        <div id="particles-js" class='col d-none d-md-block' style='height:10rem;'></div>
        <div class='container col-lg-8 col-sm-12 projects'>
            <h1 class='m-0 mt-3 d-none d-lg-block'>Dassault UAV Challenge</h1>
            <h2 class='m-0 mt-3 d-sm-block d-lg-none'>Dassault UAV Challenge</h2>
            <small class="text-muted">Under the supervision of Prof. Nazih Mechbal and Prof. Marc RÃ©billat</small>
            <h2 class='m-0 d-none d-lg-block'>Teaching a drone to detect targets on artificially generated training data</h2>
            <h4 class='m-0 mt-2 d-none d-sm-block d-lg-none'>Teaching a drone to detect targets on artificially generated training data</h5>
            <h5 class='m-0 mt-2 d-block d-sm-none'>Teaching a drone to detect targets on artificially generated training data</h6>
            <p class='mt-3 text-justify'>Please find the complete code on my <a href="https://github.com/TimothyDelille/Teaching-a-Drone-to-Detect-Target-on-Artificially-Generated-Data">GitHub Repository</a>.</p>
            <h2 class='mt-3'>Introduction</h2>
            <p class='mt-3 text-justify'>The <a href='https://www.dassault-aviation.com/en/group/news/dassault-uav-challenge-2019/'>Dassault UAV Challenge</a> is an annual competition between top engineering schools in Europe, during which a drone has to complete several missions in complete autonomy. The final objective is for the drone to autonomously explore a bounded area, detect targets of interest and deliver a package on the corresponding targets. In addition, the drone must deliver an accurate map of the explored area (see figure below). During the year 2018-2019, I was in charge of the team responsible for autonomous flight and target detection. The solution we proposed consisted in training a <strong>deep neural network</strong> (Google's Inception v2 in particular) on <strong>artificially generated training images</strong>. We mainly used the Tensorflow framework as well as the Open CV library throughout the project.</p>
            <p align="center">
                <img src="images/search_area.png" class='img-fluid' width="200rem"/>
            </p>
            <p class='font-italic w-50 mx-auto text-center'><strong>Figure:</strong> Search Area (T: Takeoff, L: Landing)<br>Three packages (red, yellow and blue) must each be delivered onto the corresponding cross, rectangles are lures</p>
            
            
            <h2 class='mt-3'>Generating Training Images</h2>
            <p class='mt-3 text-justify'>One of the most limiting factor while training object detection algorithms is the required quantity of training data. The accuracy of the detection depends on lots of factors such as the noise introduced by the drones' camera or the color of the ground during the competition. Physically taking and labelling photos of targets would offer very little flexibility with the produced dataset while being incredibly time-consuming.</p>
            <p class='mt-3 text-justify'>Seeing as the classes we are trying to learn are pretty simple in shape and color, one solution we came up with was to generate training data using the <i class='font-italic'>OpenCV</i> library in Python. The interesting aspect of this solution is that it allowed us to tinker with the following parameters:</p>
            <ul>
                <li>nuances and texture of the target</li>
                <li>nuances and texture of the background (which helps dissociating the background from the class of interest)</li>
                <li>shape of the target (imperfections e.g.)</li>
                <li>orientation and perspective of the image</li>
                <li>noise in the image (video quality)</li>
            </ul>

            <p class='mt-3 text-justify'>All those factors greatly influence the quality of the training. The following images were generated using the <a href='https://github.com/TimothyDelille/Teaching-a-Drone-to-Detect-Target-on-Artificially-Generated-Data/blob/master/create_images/create_images.py'>create_images.py</a> program inside the <i class='font-italic'>create_images</i> directory and uses some utility functions I borrowed from the <a href='https://github.com/tzutalin/labelImg'>LabelImg</a> repository.</p>

            <div class='row' align="center">
                <div class='col'>
                    <img src="images/blue_cross_training.png" class='img-fluid' width="200rem"/>
                    <p class='font-italic text-center mx-auto'>Blue Cross in its corresponding Bounding Box (little Gaussian Noise)</p>
                </div>
                <div class='col'>
                    <img src="images/red_rectangle_training.png" class='img-fluid' width="200rem"/>
                    <p class='font-italic text-center mx-auto'>Red Rectangle in its corresponding Bounding Box (important Gaussian Noise in background)</p>
                </div>
            </div>

            <p class='mt-3 text-justify'>Another great aspect of this methods is that it allows us to <strong>automatically label the training data</strong> (since we know where we generated the targets).
            <br><br>
            It might seem strange that we would deliberately add noise to our training data. <strong>How can adding noise make learning more reliable?</strong> Intuitively, the reason is that this added noise destroys inadvertent conspiracies. Thing is, overfitting happens because the learning algorithm sees some degree of conspiracies between the observed training labels and the input features. By adding noise into our images, many of these conspiracies will be masked because they are fundamentally sensitive to small details. <strong>Adding our own noise allows us to exploit variance reduction and helps generalization performance.</strong></p>

            <h2 class='mt-3'>Training Inception v2</h2>

            <p class='mt-3 text-justify'>We trained and cross-validated an implementation of Inception v2 on Tensorflow, using 9000 generated images (1500 for each class). It seems that the trained model performs pretty well on real world data, even in case of <strong>incomplete / partially hidden classes</strong>. It somehow messes up colors sometimes and seems to display a certain imbalance for yellow shapes but, this could be resolved during the post-processing phase, using a simple averaging of pixel values inside the detected shape.</p>
            
            <div class='row' align='center'>
                <div class='col'>
                    <img align="center" src="images/yellow_cross_inference.png" class='img-fluid' width="250rem"/>
                </div>
                <div class='col'>
                    <img align="center" src="images/partial_yellow_cross.png" class='img-fluid' width="250rem"/>
                </div>
            </div>
            
            <div class='row mt-4' align='center'>
                <div class='col'>
                    <img align="center" src="images/blue_cross_inference.png" class='img-fluid' width="250rem"/>
                </div>
                <div class='col'>
                    <img align="center" src="images/partial_blue_cross.png" class='img-fluid' width="250rem"/>
                </div>
            </div>
            <p class='font-italic text-center mx-auto mt-4'>The model detects partially visible targets</p>
            
            <div class='row mt-4' align='center'>
                <div class='col'>
                    <img align="center" src="images/red_rectangle_inference.png" class='img-fluid' width="200rem"/>
                </div>
                <div class='col'>
                    <img align="center" src="images/blue_rectangle_inference.png" class='img-fluid' width="200rem"/>
                </div>
            </div>
            <p class='font-italic text-center mx-auto mt-4'>The model can tell lines from crosses</p>

            <p align="center">
                <img align="center" src="images/partial_blue_and_red_cross.png" class='img-fluid' width="250rem"/>  
            </p>
            <p class='font-italic text-center mx-auto mt-4'>Even in case of poor visibility, the model can detect crosses</p>

            <p class='mt-3 text-justify'>In automated flight, we do not want to miss targets, but most importantly, we do not want to deliver a package on something else than the corresponding target. That is, we want to minimize the number of <strong>false positives</strong>. In order to do so, we compute a <strong>moving average on the \(k\) last images</strogn>. If the moving average crosses a threshhold we fix at \(0.8\), we can safely assume we rightfully detected a target.</p>

            <p align = "center">
                <img align="center" src="images/moving_average.png" class='img-fluid' width="350rem"/>
            </p>
            <p class='font-italic text-center mx-auto mt-4'>Figure: Plot of the moving average of detections we compute for each class during the flight</p>

            <h2 class='mt-3'>Automating the Single Delivery Mission</h2>
            <p class='mt-3 text-justify'>Now that we have a satisfying model for detecting targets during the flight, we have to automate the process of exploring the area. The trajectory of the drone is determined by a takeoff and landind position (GPS coordinates).
            <br><br>
            The mission should elapse as follows:</p>

            <ol>
                <li>
                    Arm and Takeoff
                </li>
                <li>
                    Begin flying towards first waypoint
                </li>
                <li>
                    While True:
                    <ul>
                        <li>
                            Update the map
                        </li>
                        <li>
                            Detect candidates from video stream
                        </li>
                        <li>
                            If candidate:
                            <ul>
                                <li>
                                    Check that the moving average is above the threshhold
                                </li>
                                <li>
                                    Compute the position of the target (center of the bounding box converted to GPS coordinates relative to the current position of the drone)
                                </li>
                                <li>
                                    Go over the target
                                </li>
                                <li>
                                    Go down, deliver package and go back up
                                </li>
                                <li>
                                    Go to landing position
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ol>
            
            <p class='mt-3 text-justify'>In order to dynamically display the explored map, we developped the *display_map.py* module using the *Tkinter* library, where we can monitor the altitude, position and detections of the drone.</p>

            <p align="center">
                <img align="center" src="images/single_delivery.png" class='img-fluid' width="400"/>
            </p>
            <p class='font-italic text-center mx-auto mt-4'>Left: processed video stream from the drone<br>Middle: display map window<br>Right: terminal</p>

            <p class='mt-3 text-justify'>The drone goes over the target and detects it:</p>

            <p align="center">
                <img align="center" src="images/display_map1.png" class='img-fluid' width="300"/>
            </p>

            <p class='mt-3 text-justify'>And delivers the right package on the detected target:</p>

            <p align="center">
                <img align="center" src="images/display_map2.png" class='img-fluid' width="300"/>
            </p>

            <p class='mt-3 text-justify'>Before going back to the landing point.</p>

            <h2 class='mt-3'>Mission accomplished!</h2>  

        </div>
        <div class='container mt-5'>
                <p>Feel free to contact me at <a href='mailto:timothydelille@berkeley.edu'>timothydelille@berkeley.edu</a>.</p>
                <div class='row text-center w-25 mx-auto'>
                    <a class='col'href='https://linkedin.com/in/timothydelille'>
                        <i class="fab fa-linkedin fa-2x"></i>
                    </a>
                    <a class='col' href='https://github.com/TimothyDelille'>
                        <i class="fab fa-github fa-2x"></i>
                    </a>
                </div>
        </div>
        
        <!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    </body>
</html>

<script src='../scroll-fade.js'></script>
<!-- scripts particles-->
<script src="../js/particles.js"></script>
<script src="../js/app.js"></script>